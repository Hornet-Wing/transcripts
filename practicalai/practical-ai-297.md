**Daniel Whitenack:** Welcome to another Fully Connected episode of the Practical AI podcast. In these episodes Chris and I try to keep you fully connected with everything that's happening in the AI space, and hopefully share some things that will help you level up your machine learning game. I'm Daniel Whitenack, I'm CEO at Prediction Guard, where we're deploying a platform for private and secure AI, and I'm joined as always by my co-host, Chris Benson, who is a principal AI research engineer at Lockheed Martin. How are you doing, Chris?

**Chris Benson:** Doing very well, Daniel. I'm podcasting from outside today...

**Daniel Whitenack:** That's exciting.

**Chris Benson:** It's a cool November night, but since I've just moved house and I don't have a place to sit, nothing but boxes here... We're talking about AI outside today. This is an outside AI day.

**Daniel Whitenack:** Yeah, yeah. You live in a place where it's possible to be outside reasonably comfortable in November...

**Chris Benson:** That's right.

**Daniel Whitenack:** ...right before Thanksgiving. Yeah, it's a bit colder up here in the Midwest. We're definitely getting to that Midwestern time when the Carhartt jackets come out, and the beanies... Yeah. It's a good time of year. It means Thanksgiving is upon us.

**Chris Benson:** That's right. We've got tofu turkey coming up here.

**Daniel Whitenack:** Tofurky is imminent. Forthcoming. Exciting. There's better ones than other ones... So this isn't a podcast about tofu turkey, or tofurky, but there's some that are better than others, and we'll maybe let people hop on their own minds if they're exploring that territory.

**Chris Benson:** We need some AI-generated tofurkys coming at us.

**Daniel Whitenack:** There's got to be some intersection.

**Chris Benson:** That's right.

**Daniel Whitenack:** Maybe tofurky is using AI to generate ad copy this year... Which reminds me - I don't know if you've been seeing all the things in the news, Chris, about Coca-Cola's ads, AI-generated ads... Have you been seeing any of that? Have you seen the actual ads?

**Chris Benson:** I have not seen the actual ads, but I have seen some of the news talking about it.

**Daniel Whitenack:** Yeah, yeah. So for those that aren't aware, Coca-Cola - you know, every year Coca-Cola kind of creates these iconic Christmastime ads, with the Coca-Cola truck, and the polar bear, and things like that. And this year, at least -- I don't know if it's all the ads, but there's at least one ad... I haven't been following the exact details, but there's at least one ad that is fully AI-generated, or at least driven by AI-generated video clips, or images, that sort of thing. And I've seen it on the streaming services. I forget which ones, whether it's Prime, or... They sort of all have ads now, because it's basically like cable at this point. But all of them have ads, so I've seen the Coca-Cola ad on the streaming services, and... Yeah, I think maybe those that haven't seen it out there should go watch it. I think it's interesting that there's certain elements of it that give you that AI-generated vibe, where you could kind of tell... But it definitely evokes the character of the sort of Coca-Cola ads. And lots of people don't like it, lots of people think it's interesting... Some people on LinkedIn I've seen said "Well, if AI-generated video is good enough for Coca-Cola's Christmas ads, then who is it not good enough for at this point?" Which is maybe a hot take. I don't know. Any thoughts, Chris?

**Chris Benson:** I'm just kind of amazed that people are surprised by that these days. It's like, you're going to see this stuff everywhere. And so - yeah, okay, iconic thing, I got it... But yeah, I mean, I would have been almost surprised if they hadn't.

**Daniel Whitenack:** \[00:07:39.02\] Yeah. And yeah, if you just search for Coca-Cola ad, I think it's the "Real Magic Holiday" ad, which is also a bit ironic that they titled it Real Magic... When it's definitely not real. But yeah, you can watch it. It's pretty interesting. Whether or not it's really, really good ad material, it's I think a sign that for sure AI-generated video is here with us for the future.

**Chris Benson:** You had a few some months back the actors going on strike... But I just think that it's one of those things we have a long way to go; not just in entertainment, but in most industries, where it's going to -- you're going to see corporate videos that are AI-generated. I've already seen that. I may not have seen the Coke one, but I've seen corporations that are doing it. It's the way it is now.

**Daniel Whitenack:** Yeah, certainly companies like Synthesia and Haygen and these video generation companies for training videos, for multiple languages, all these sorts of things... There's a lot of use of those. I've definitely seen it.

**Chris Benson:** Disruption.

**Daniel Whitenack:** Yup. Speaking of disruption...

**Chris Benson:** Oh, boy...

**Daniel Whitenack:** We haven't talked about this yet on the show, Chris, and I don't think either of us have a desire, nor maybe, at least on my part, any sort of profound opinion on this topic, other than the fact of what it means for AI. But I saw an article in Time about what Donald Trump's win means for AI. So if you're if you're listening to this podcast at a time sometime in the future when it's not election season, maybe you're looking back on this and you know what Donald Trump's second term meant for AI... But at this point, we don't necessarily know, although we could make some guesses which we can talk about... But yeah, we're about to go into the second Trump administration... So if you're listening to this at some other time, that's the time that we're talking about this.

And yeah... Interesting. We've seen - maybe just as a reminder, we've seen the Biden administration do some things as related to AI, including the executive order on AI, which we did talk about on this show... That was episode 244. So if you're wanting to know -- if we refer to that and you want to know the details and the interesting pieces of that executive order, that's episode 244, which we'll link in the show notes. But yeah, interesting. Any initial takes on, as a practitioner, what this means for us?

**Chris Benson:** I can tell you what I hope it means... And I hope -- you know, during the first Trump administration he didn't know very much about it. He brought in some corporate folks to put together some committees, and there was a little bit that came out of that, a website, and stuff like that... But it didn't impact us too much at the time. So I think part of me hopes that maybe it will be gentle. Let him talk about rolling other things back, but maybe he's not aware enough of AI to do it. But of course, it's been another four years, and who knows where that's going? So a little bit nervous to see where his policies take us. But I hope he's more or less hands off.

**Daniel Whitenack:** Yeah, yeah. So in the Time article - this is a quote from that article, which we can put in the in the show notes... It says "Trump's own pronouncements on AI have fluctuated between awe and apprehension." Describing it as a superpower or very alarming, right?

**Chris Benson:** Often in the same sentence.

**Daniel Whitenack:** \[00:11:42.06\] Yeah, maybe so. But one of the things I think that has been kind of promised, maybe as a part of just undoing some of the things of the Biden administration, which I think we can expect more generally, is a promise to repeal the executive order on AI, among probably other things... And I think citing the hindrance of innovation, this kind of anti-regulatory take on a lot of things... So there's a promise to repeal that. I'm not enough of a lawyer/politician/political analyst to know what exactly that undoes, because the executive order, I think, kind of has its tentacles in a variety of things that it touches, that are maybe not immediately related to the executive order, like the NIST, AI risk frameworks, and those sorts of things... So I don't know exactly how that works out. Maybe that's a point of confusion on my part.

**Chris Benson:** Yeah, my concern is - you know, there are some things that I think if you didn't just have a knee-jerk reaction to anti-anything that Biden did, that there are actually some things that the current administration and the incoming administration should be able to agree on. And one of those that's not AI, just as an example, is the CHIPS Act, which is kind of trying to bring semiconductor capabilities back online in the U.S. And if you are kind of -- if you're an administration that's anti-China, or in the China/Taiwan concern, which Trump has said he is, you would think that that's actually something that both sides of the aisle could agree to. But he's also said he's going to repeal the CHIPS Act as well. And I fear that the executive order, since it is something he can repeal with just the stroke of a pen, might suffer that. And yet, I think that he would be making a mistake regarding his own administration. I think that would create problems.

**Daniel Whitenack:** Yeah. The article that we're referring to even talks about this, that there's some statements about, you know, we're going to need more chips, and more chip production... But at the same time, as you mentioned, the Trump campaign has attacked the CHIPS Act. Certain things, of course, are still in progress that would, I think, fit the America first chip production piece of that, including -- I just saw in the news that Intel was awarded up to almost 7.9 billion under the CHIPS Act, to help build or expand chip plants in Arizona, New Mexico, Ohio, and Oregon, including 1 billion plus later in 2024. So some of this I know, especially the Ohio plant and all of that, is I think in progress. I don't know the exact details of that. But yeah, some of this is in motion... So it is a bit confusing to me. I'm sure that CEOs of large companies are on the edge of their seat and trying to get audiences with the right people, and understand what's going on... I'm assuming -- again, I don't know how all of these things work under the hood, but I'm assuming there's a lot of that shuffling going on to get a read on the situation.

**Chris Benson:** Yeah, I would hope that if there's anyone out there listening that might be a part of the incoming Trump administration, making America great again is exactly what the CHIPS Act was intended. And frankly, I think the AI executive order does the same. So I'm hoping there's no knee-jerk on those two things, despite the comments. Maybe he'll let them continue.

**Daniel Whitenack:** Yeah. What is your take on the potential perspectives on open source, or closed, in the Trump administration? Any thoughts on that in terms of how that may be influenced one way or the other?

**Chris Benson:** \[00:16:03.13\] I don't really know at that point. I think it depends on who's in the cabinet, potentially, and probably more specifically who's working on staff at the White House, and what their takes on it are. And I couldn't speak to that.

**Daniel Whitenack:** Yeah, I've seen a mix of takes on that... I think there's one perspective that while China has benefited greatly from open source AI, not only have they been model builders and actually producing a lot of technology in the AI space, but they've also benefited a lot from meta and US AI technology... So there's kind of one side of it that would be "Well, let's lock that down", in the same way that they might try to restrict exports of other things, or that sort of thing.

But I've also seen the other take on the fact that you're basically anti-regulation, and it would be kind of not within the character to be restrictive in terms of the open source AI world. So I think it's a little bit unclear. I'm certainly one that kind of views the future even more importantly in security, privacy-conscious industries really driven by open, self-hosted models. I think that's really the way that you ensure security, privacy, transparency.

**Chris Benson:** Yeah. I think there's a lot of ambiguity at the moment, because if you look at traditional conservatism, if you look at Ronald Reagan... Because a lot of Republicans really look back to that. Open trade is huge, but we're also having Trump talking about tariffs. That's been the news of the week. And that's kind of the antithesis of that. So it's kind of hard to figure out where the ball's going to land on those.

**Break**: \[00:18:06.26\]

**Daniel Whitenack:** One thing that was kind of brought up in the midst of this talk of the Trump administration and AI is this sort of AI and China discussion, where there's a thought AI is kind of thriving in China, and maybe China is pulling ahead in AI... I know we've talked about this on the show before. There's kind of this discussion of China and AI every time policy decisions are discussed on this show, and kind of factors in... And one of those things that I think is relevant is just the dominance of Qwen-based models in recent times. So if people aren't aware, one of the things that I think is interesting to follow recently is Alibaba's Qwen family of models. That's spelled Q-W-E-N, Qwen. The latest of these is the Qwen 2.5 model family. And generally, these Qwen 2.5 models are quite impressive. They generally top the open LLM leaderboards in various categories. You'll see them in the top spots.

So obviously, these are Chinese models, that is they're models being built by a Chinese company, Alibaba. The CEO of Hugging Face, Clem, is quoted in one article I was reading of "Qwen 72B is the king, and Chinese models are dominating." That's a pretty clear statement. That was earlier in the summer, but I think we've seen continued domination of these models.

Any interesting takes on that, Chris, in terms of how you've seen the model landscape shift from closed model providers, to open, to maybe more geographically diverse, and certainly China being within that?

**Chris Benson:** I'm in an industry, I'm in defense and intelligence, where obviously we're not going to be using Chinese models... And so we have not been focusing on that. We, of course, keep track of everything out there, but that's not one we're likely to use. But I'm really curious, outside of the sector that I'm in, I'd love to get some feedback from people on what they're uptaking. I think there are a lot of industries where they're not going to care either way on that, and they're going to go for the best models on the leaderboard. But I haven't actually talked to anyone who's done uptake. How about yourself?

**Daniel Whitenack:** Yeah, and maybe this is an interesting little diversion here, because I think some people don't understand the potential security risks as associated with this sort of model. So we say it's a model produced in China, some people would be uncomfortable because of China's use of data, or ways that they would use this technology... But if we look at the model itself - so you can go to Hugging Face and just search for Qwen models. So the Qwen models are open in the sense that you can go to Hugging Face, it's a repository of models... You can literally go to the Qwen model. You can download the weights of the model, and load that model into infrastructure that you control. So this model, when you think of the model, is composed of parameters and model code that runs that model. So if you go to the model on Hugging Face, you can download that.

\[00:23:48.11\] Now, similar to like if you were to go to GitHub, and you look at all of the repositories on GitHub, some of those repositories on GitHub will have security considerations or licenses that won't allow you to use them, or sources that you don't trust... It's a little bit interesting here, because these models are kind of loaded into code that is maintained by Hugging Face, the Transformers library, or other serving frameworks. So if you're self-hosting the model, meaning you're pulling the model down from Hugging Face, the files, and you're loading it into code that can serve that model, that model serving is under your control and you're downloading those files, meaning you can inspect them. It doesn't mean there's no security vulnerabilities associated with them, but ultimately, all of that is under your control.

That is a different scenario than if you were to connect to an API that is serving the Qwen model, which there are ones from Alibaba and others, where this model is actually hosted as a product of a Chinese company. You're sending your data to that API product, which is then processing your data, and giving you a response back from the model. So I just wanted to emphasize there's kind of these two scenarios here. So one, in one scenario, the security vulnerability is really related to the model files that you're downloading. Is there any security vulnerability in those model files, which there could be? Is there any third-party code that's used when you load those model files, which there could be? And what serving framework are you using to serve them, which could have security vulnerabilities?

In the other case, you're relying on someone else's infrastructure, which isn't under your control, which might be under Alibaba's control. So these are just different concerns that you want to weigh... And I thought that may be good to highlight, because some people may even want to experiment with the Qwen model, in a thing like LM Studio or something like that. I'm not vouching for all the safety considerations that might be in your mind, but it's not like, I don't think, when you use Qwen in LM Studio there's some sort of phone home to Alibaba going on necessarily in the underlying code that's running that.

**Chris Benson:** I think in U.S. government circles, just to clarify something, I think it's more policy than necessarily... So I think you're going to have some agencies that are downloading all the models, and reviewing and inspecting and stuff like that... But I think for typical usage, I think you're much more likely to see a U.S. agency or corporate that is serving the U.S. government going to be focusing on Meta versus Alibaba. I think that's just a policy issue.

**Daniel Whitenack:** Yeah, yeah, yeah, for sure. I think you're right. I've just seen a lot of confusion around this. It's like --

**Chris Benson:** No, it's a good clarification.

**Daniel Whitenack:** Anytime you use a Qwen model, it's stealing your data... But there may be ways to use this in a way that is appropriate for your scenario.

**Chris Benson:** Sure.

**Daniel Whitenack:** Likely, like you say, if you're working in defense or something, that's going to be a different consideration than if you're hacking together a cool AI agent on your side project for personal purposes. Those are very, very far apart on the spectrum. So yeah, very, very interesting though.

Also, there's some recent development... So we're late in November already, but this is I think about a week ago, something like that... Quinn Turbo 1 million was released, a sort of new version of this, which extends the context length of the Qwen 2.5 language models from 128k to 1 million tokens. So that's -- to kind of give a context, some of what's cited is like 150 hours of transcripts, or 30,000 lines of code, or these sorts of things. So lots of context can be put into these models, which is a trend that has continued, and I have my own opinions about, but it does seem to be a trend that continues.

**Chris Benson:** \[00:28:06.22\] Go ahead and share them. You can't hang that out there and not go there now.

**Daniel Whitenack:** Yeah, well, I just think if you think about the typical, the most common enterprise cases that I run across in working with customers, most often these fit these scenarios of what I like to think of as something that could be done by a college-level intern, right? So you have some very clear instructions to do this sort of workflow... And it might be multi-step, it might be a complicated workflow, but it's all -- like, you can break it down in a sequence. There's instructions there. So anecdotally, if you go to a college level intern and you say "Go into the warehouse out back. There's rows and rows of documents. Now do this task for me." Right? That's a much harder thing, with a higher degree of potential failure, than if you go to the warehouse and you find generally the section that's relevant to a task and you say "Hey, look at these couple folders of documents and do the task." You're much more likely to get a better result, and I think these models anecdotally behave similarly... And there's some evidence for this in terms of the forgetting of what's in the middle of the context, which has been observed in academic research... And I'm sure people on this podcast will be like "No, Daniel, that's solved", whatever. It's just my own sort of experience and anecdotes in terms of what has been found to be useful. Yeah, a million tokens is a lot, so...

**Chris Benson:** Possibly more than most people are going to need, and...

**Daniel Whitenack:** I know people that have been on this podcast and are peers of mine that totally disagree with what I've just said, so that's okay. We're all kind of figuring it out as we go along, I guess.

So Qwen 2.5 - that intersects with some of our discussion around the China-America debate, but there's a variety of models that people might be interested in just taking a quick look at, that have popped up over the last weeks... And it's been a while, Chris, since we've done a "Here's a buffet of new models" type of brief disclosure... And there's a few interesting ones.

So, there's one that is from DeepSeek, which previously released a series of really good coding models... But they've released DeepSeek R1 Light Preview, which is kind of fitting in this ChatGPT 01, or OpenAI 01 kind of world, which is this kind of pause and think about things sort of world, where it's trying to solve very complicated math benchmarks, or other things... And so you see actually this DeepSeek model in many cases for certain benchmarks - it may be even doing better than 01 Preview, in a number of benchmarks... So I think this is further evidence that this gap between the closed model providers at the frontier and open model providers is just closing so rapidly. It's, in my opinion, basically not distinguishable anymore, in a lot of things that people want to do, whether you want to use a open model or closed model.

**Chris Benson:** So let me ask a couple of questions around that. Number one is - you know, we've seen so much in the news about kind of hitting the limit lately. OpenAI has come out, and talked about delays on future models, because they're kind of hitting practical limit... People have left the organization as a result of that... And just in general, that's been the conversation in industry over the last month or two.

\[00:32:14.19\] As we do that, do you think that this is kind of the place that we're going to continue to see models evolving into, where instead of just getting bigger and larger context windows, and the whole thing, all that, always bigger, always better, that we're starting to see these kind of -- these 01 Preview styles, where they are pausing and they're bringing whole new techniques in to tackle certain types of problems? Are we maybe going down that path, as well as others?

**Daniel Whitenack:** Yeah, yeah. From my perspective, at least, one thing that's happening is the gains that are being made from more data, and larger models, have basically plateaued... Which has been observed. Which means that smaller models that people are doing a lot of work to curate data for, and innovate in terms of their efficiency, are catching up rapidly to the larger models. So what would have been only possible by a 70B model, or a 400B model even six months ago or three months ago, is being done by 7B models or smaller. So you've got this small model trend where these models are actually performing at levels much higher than what was able to be seen before.

And then you have kind of branching out to various both specializations or domains, and kind of unique prompting or formatting skills. So domains like document parsing, or vision, and that sort of thing... Hugging Face just recently released the Small LVM, which is a small model that does sort of vision-related activities. There's the OutTTS, which is a really efficient, 350 and 500-million parameter text-to-speech model... Both of those I think represent this kind of specialization of smaller models, and doing really well at specific things.

And then I think you will see kind of an attempt to continue to develop new types of fine-tuning and prompting methodologies for things like this deep thinking, and for things like agent-related workflows, which I think people are going to be diving into more. So it may be more about the workflow, the prompting format, the prompting strategy as we move forward for just pure text models, than bigger and better models, bigger and better datasets.

**Break**: \[00:35:01.29\]

**Daniel Whitenack:** Well, Chris, speaking of a couple things that I think are pretty cool, and maybe even practical, that we could share as people are trying to level up things... One which is just fun, which is on my list of things to try this week is something that I've found - or someone pointed me to - which is called Pickle. Granted, I haven't tried this yet. I've actually just found it today. But it's not the Pickle -- if you're a Python programmer, Pickle means something very specific, which is a serialization format. But if you're not a Python programmer - yeah, if you just go to getpickle.ai, this seems like what I've been waiting for for a good long time...

**Chris Benson:** I'm looking at it now.

**Daniel Whitenack:** ...which is just a pretty good catchphrase, "Join meetings with clone." That's all I've pretty much wanted to do for quite a while. The idea is basically you would have a professional-looking video, and you could be laying in your bed, without any pants on, and your headset on, and your audio would be going through your clone into a very professional-looking person that has joined a Zoom call, or whatever call, but you don't have to ever put any pants on, or that sort of thing. Or you're driving, but it looks like you're in your office, right?

**Chris Benson:** It looks great to me. I'm all in on this thing.

**Daniel Whitenack:** Yeah, so... Super-interesting. I mean, I don't know what this sort of thing, along with other things like AI avatars and all of this, means for the relational elements of work... What I was thinking when I saw this was "Well, can I go a step further and just generally instruct a language model to generate my responses, and only just sit there, listening to my clone?" I just want to sit there, listening to my clone, while my clone does the meeting for me, and then just interject or kind of interrupt my clone, and take over my clone's mind in the meeting, when I need to correct something, or jump in.

And otherwise -- because most of the time... I don't know about you, Chris, but most of my meetings are like "Hey, we're going to go around and introduce everyone." So no problem, my clone can introduce me. And then you can go around and be like "What's your update on this project?" Paste in a document, give an update. There's really not a lot of things that I do in meetings. Maybe this is going to get me fired or reduce my value at work...

**Chris Benson:** You're your own boss. You don't have to worry about that.

**Daniel Whitenack:** Yeah, there are important things, occasionally... But yeah, I'm kind of wondering when that happens.

**Chris Benson:** I'm just thinking out there in corporate world, all the status meetings that people go to, where you're just bringing your status and you're basically - exactly what you said; you have your status written down, you've kind of already pre-trained it to the introduction, all that... You can kind of lay there half asleep in bed, let it just handle your turn when that comes... The only thing you've got to worry about is if somebody starts asking questions outside the context of what you can train. If someone takes a right turn, you've got to be ready to leap in. But I could see lots of my meetings being taken over by this capability. I would happily do that, too.

**Daniel Whitenack:** And I don't know -- let's take a stand-up, for example. An engineering team's stand-up, or something like that. Part of the idea behind such a thing, I think - I'm not a scrum master, but part of the idea would be to also actually hear with your ears other people's update, and maybe that influences... Either they're blocked on something, and you can reply, or it influences... So I'm wondering what this does, if it creates more potential isolation in an already remote work distributed environment. And part of me -- so I have a friend, Mark Sears, shout-out to Mark, if you're listening... He's working on a venture studio called Sprout AI. And one of the things that is one of their theses is that they wanna build technologies with AI that drive people relationally together as people.

\[00:41:52.25\] So the idea, just to give an example, would be like, Chris, you and I, maybe we're friends, we're both busy, we're professionals... And so there's an AI assistant that maybe looks at your calendar and looks at my calendar, and looks at events going on in our town, or things that fit both of our interests, and then messages us both and says "Hey, Thursday night you're both free, and there's this event in your town. Are you guys --" And that's a sort of thing that is cool. It kind of drives people relationally together, it gets them out of their house.

I think this idea of sort of embodied AI that would drive people relationally together is very appealing in our day and age, and something that's needed. But I also love the idea of joining meetings with my clone. So I don't know how to bring those together, but...

**Chris Benson:** I told you I'm all in, but going to your talk about kind of driving humans out to have real connections and stuff - I just have this vision of it kind of taking over the dating world. I'm a long way removed from that. You and I are both happily married men...

**Daniel Whitenack:** I didn't think about that, Chris, but yeah.

**Chris Benson:** But no, I'm just having this vision of this single guy and single woman, both are in a bar, but neither one's very comfortable, and they send their agents to connect.

**Daniel Whitenack:** They send their agents to screen?

**Chris Benson:** That's right. The agents screen each other and decide whether or not it's a green-green, or a green-red, figuring out... And I can just imagine -- my daughter is too young to be dating, she's 12. But I could imagine 10 years down the road, her having one of these agents, and finding her boyfriend by letting the agents check each other out. So who knows where it's going?

**Daniel Whitenack:** Yeah. And in their little video on their site, they have a picture of a woman holding her baby, and she's on the phone, joining the meeting with a clone... So I could definitely see various lifestyle elements of this, where there could be a stigma with you joining a meeting... You know, your spouse isn't there, you have to deal with your baby at the time, you're working from home... And that may not be something that either you're comfortable, or that would be accepted, unfortunately, in certain scenarios... And so yeah, I definitely see elements of this, but also, I wonder about the kind of isolation driving forces of all of this.

**Chris Benson:** There is a really good point there. And just for a moment, stepping back out of the AI-driven meeting concept, if we step back a few years to when COVID was hitting and we were all kind of just making do, and having remote meetings, we became much more tolerant of one another in terms of how your business life intersects with your personal life. And if the dog was barking in the background, people learned to be just fine with that. And if there was kids, or a baby, people learned that. There is an element of this, as we're talking about this particular thing about having that clone out there, of kind of going backwards on that trend, and us being a little bit less tolerant of one another, because you're, once again, projecting that perfect image, whether you're in the car, or on the toilet, or in the bed, or whatever it is that you happen to be doing that you don't want to reveal. This is one of those things. It could be isolating to use it in that way as well.

**Daniel Whitenack:** Yeah, interesting. I think it will be interesting to see how people leverage these both ways. And like many things we've seen with this technology, there are opportunities for sort of restorative, positive, redemptive kind of uses of this technology, and there's ways that it can kind of drive us into isolation or create issues.

\[00:45:56.22\] But yeah, along that front of kind of lifestyle-related things happening with AI, I've seen a couple of posts recently related to kind of payments and commerce and shopping in AI... The first of those being a blog post from Stripe, which talks about adding payments to LLM agentic workflows. And I guess there's better tooling now to the Stripe Agent Toolkit, which is - if you go to github.com/stripe/agent-toolkit, you can now kind of plug in Stripe as a tool or as a thing that can be leveraged by AI agents, including those from LangChain, CrewAI, Vercel's AI SDK, which - it's definitely pretty cool. It's that kind of scenario like "Hey AI, I need you to book a rental car for me next week." And obviously, that requires some sort of payment. I could also see it on the other end. Being a business owner right now, I'd love to say "Hey, create a recurring invoice for this customer, for this amount, with these line items, and send it to them with a message saying..." Whatever those things are. There's definitely a room for maybe misuse or problematic things happening here, but certainly very, very interesting to see this side of things advance.

**Chris Benson:** It is. I think it's a great thing, personally, the concept of an agent. I know it'll take people time to trust it and get used to it, but I know in our household at this point we tend to buy our groceries and have them delivered and stuff, because we're busy and doing stuff... And a lot of times it's the same stuff as you bought last week, but maybe with a few changes, because you're planning a different type of meal at some point during the week... And I think if you can combine the agent with the payment capability, and have the ability to kind of just smooth your life in that way... I know our family would love that. My wife would absolutely. She'd go nuts for it if that was available. She'd be like "Yup, I'm offloading that. Agent gets it all."

**Daniel Whitenack:** \[00:48:17.23\] There's another -- I don't know if they're using the Stripe API under the hood, but there's another entrant into this, which is Perplexity now offers a sort of shopping assistant with an actual experience behind it kind of built in. So you have the ability to put in like "Hey, I'm doing this project, and I'm wanting to do this and that. What are the items that I need? Help me kind of shop for those." I think that's kind of the vibe. And there's a search that happens, obviously, and it's plugged into various products... In this case, they have a merchant program, which definitely seems... So I don't know whatever happened to kind of some of the monetization around like plugins and other things with ChatGPT, but this definitely seems like a way to kind of get your product... You know, having a wife that owns a business in the direct to consumer space, and sells products direct to consumer, there is this element of trying to figure out "Well, how do I place my product, or how does my product kind of filter up into search results when people are just searching on ChatGPT, Perplexity, whatever?" And so this does seem to be one angle on that, where you can increase chances of being a recommended product, there's payment integrations, API, custom dashboard etc. So there's this sort of merchant program element of the Perplexity AI-powered shopping assistant as well. Pretty interesting.

**Chris Benson:** Very nice. I'm looking forward to all of it. Let's just adopt now. I'm ready for all of it. Go.

**Daniel Whitenack:** Yup. Well, as people build out their shopping assistants with the APIs from Stripe or others, or if you're building your own things... Here at the end of our show, we normally try to point people to a couple of useful things. And I'll just mention a couple very seemingly useful things that I ran across in the past couple of weeks.

One of those is called Docling. You can just search for docling, and we'll put it in the show notes as well. So this seems to be a really nice toolkit that a lot of people I've seen mentioned, related to document parsing, which is a really hard thing generally, and a hard thing to get right in a lot of AI workloads. And there's some custom models that have been built around various complicated document parsing situations. So this kind of is a standardized way to parse PDFs, and PowerPoints, and images, and Excel documents and other things, and get them into a standardized format.

The other one that I saw, which was pretty cool, is called Observers. I'm a big fan of DuckDB, Argilla, HuggingFace datasets, all of that sort of tooling... And this is plugged into all of that, and allows you to kind of suck in all of the requests that you're making to various AI API providers, or your own models, and save those in something like DuckDB, or Argilla, or something, for the future of kind of searching through a history of prompts, but also utilizing that either for just observation and transparency and logging and debugging, but also maybe for eventually open source datasets around prompts, or even fine-tuning datasets in your own context. Both of those pretty interesting new projects, check them out. But yeah, this has been fun, Chris.

**Chris Benson:** Good. I learned a lot today. I appreciate you bringing some of this stuff.

**Daniel Whitenack:** Yeah, good to chat. We'll talk to you soon.

**Chris Benson:** Take care.
