**Daniel Whitenack:** Welcome to another episode of the Practical AI Podcast. This is Daniel Whitenack. I am CEO at PredictionGuard, where we're building a private, secure gen AI platform, and I'm joined as always by my co-host, Chris Benson, who is a Principal AI Research Engineer at Lockheed Martin. How are you doing, Chris?

**Chris Benson:** Hey. Doing very well today, Daniel. How's it going?

**Daniel Whitenack:** It is going great. I'm super-excited about this one, because it's a very -- you know, we schedule a lot of shows, and they're all interesting, of course... But occasionally, there's a show on a topic that intersects with something that I'm working on at the moment, or something that I've found that's really exciting, and found to be really useful... And so selfishly I'm really extra-excited about this episode this week, which is with Till and Adithya from MotherDuck. How are you doing?

**Adithya Krishnan:** Doing good. Excited to be here.

**Daniel Whitenack:** Yes. And note, duck as in a bird. So editors, you don't have to bleep us out... Sure, that's something that is an old joke for you all. I can pinpoint very easily how I ran across DuckDB and MotherDuck - there was a blog post... The title was very simple. It said, "Big Data is Dead." And immediately when I saw the title, I was like "Thank goodness. Finally." But I'm wondering if you can maybe just kind of step back... It doesn't necessarily have to be the points in that blog post, but how you see the kind of data analytics, big data, AI intersections as of now, and what are the sort of concerns and issues that people are thinking about that is driving them to DuckDB? And then, of course, we'll obviously get into DuckDB and MotherDuck and all that you're doing. But setting that stage of what are people struggling with? What have they realized in the past about this sort of big data hype, in one way or the other, positive or negative? And how has that kind of changed the way that people are thinking about analytics and databases?

**Till Döhmen:** I can tell a story about how I got in touch with DuckDB... It started at the very beginning of the DuckDB project. I was actually doing my master's thesis back then at the CWI, where DuckDB originated from... And after I graduated, Hannes, who is the developer or the founder of DuckDB Labs, reached out, and we were talking, and they were saying "Hey, we're working on this new project. We're working on this database system. Are you interested in maybe joining, maybe working on it?" But I was very focused on machine learning and stuff like this, so I wanted to go into data analytics, data science, these kind of things...

So a year later or so I was working at a tech company, and we were analyzing customer data with Spark, and so on... And one day one of the first versions of DuckDB was released, so I pip-installed it, and I ran a first simple aggregation query on maybe a 100-megabyte dataset, or something like this... And I was surprised, because I thought something was going wrong. I thought "It's impossible that it just did the aggregation", because from working with Spark, I was so used to "Okay, now spinner's turning for 10 seconds at least." That was really eye-opening, and I've heard a similar experience from a lot of people... Even until today, I hear very similar stories and experiences.

**Adithya Krishnan:** \[00:08:06.22\] Yeah. For me, it started in a different way. I first figured out DuckDB Wasm existed, that you could run an analytical engine in the browser... And to think about something like that was super-crazy... And the kind of stuff that you could do on top of it started to look super-crazy. And one of the things that I was super-excited about when DuckDB Wasm released was the possibility to do geospatial analytics. So back then, when I started, my first encounter with DuckDB was doing geospatial analytics. And then to think about that could actually be done in the browser was mind-blowing. And that's when my journey into DuckDB started.

**Chris Benson:** So let me ask you all a follow-up question as you're diving into your passion... For those out there who may be listening who are not already familiar with it, and they're hearing database, they're hearing "Big data is dead", they're hearing doing this in the browser... Give me a little bit of background on kind of the ecosystem that you were coming from a bit, and also what this idea was, so that people can kind of follow you into that. What is it that caught your passion and attention and made you say "Ah, this is the way"? And assume somebody doesn't already have a familiarity with it.

**Till Döhmen:** So I was going into this coming from the machine learning side of things... So I was used to working with scikit-learn, Pandas, or the Spark equivalents to that, like SparkML, building data prep pipelines, and so on and so forth. And then encountering this DuckDB thing, suddenly, that apparently is doing aggregations of the sizes of data I was working with much, much, much faster... Yeah, it sparked some fantasies around "Hey, how much of the data preparation pipeline can we push into DuckDB, actually?" And this idea or this fantasy has been following me for the past years, and I think it's still an exciting topic.

**Adithya Krishnan:** To follow up a little bit on that, the way that large data or big data has been analyzed in the last years - I mean, predominantly that you required some server in the cloud, you required resources that were not local to be able to perform large analysis... But something that DuckDB opened up, that made possible to use local compute in your local macBook, for example, was to utilize that compute at the most to perform this kind of huge analysis. And that, I guess, set Spark to a change in the ecosystem, I would say. And I guess that's where we're at.

**Daniel Whitenack:** I resonate so much with this... Coming from a background also as a data scientist, living through the years of being told "Hey, use Spark for this." Basically, my experience in this ecosystem was I would try to write a query, and it would get the right result, but to your point, Till, I would just be waiting forever to get a result. And so I'd have to send it to some other guy whose name was Eugene. Eugene was really smart, and he could figure out a way to make it go fast. And I never became Eugene. So I resonated with this very much.

And the fact that this concept of "Hey, there's these seemingly big datasets out there", and I want to do maybe even complicated analytics types of queries over these, or even execute workflows of, as you mentioned, Till, aggregation, or other processes at query time, I could do that with a system that I could just run on my laptop, or I could run in process is really intriguing.

\[00:12:03.01\] So maybe now is a good time then to introduce DuckDB formally. So I'm on the DuckDB site. It says "DuckDB is a fast, in-process, analytical database." So maybe one of you could take a stab at thinking about those data scientists out there who are maybe at the point of also not believing that what we just described is maybe possible, or they're living in a world where that's not possible... Describe what DuckDB is, and maybe why that becomes possible as a function of what it is.

**Till Döhmen:** I think I can talk a little bit about the motivation behind DuckDB, or at least the way I perceived it at the time. That actually originated from the R ecosystem. So Hannes was very involved in that ecosystem, and people were using R to essentially crunch relatively large data, with relatively primitive methods. And so at the time, CWI had a database system and an analytical database system called MonetDB, that has incorporated the idea of vectorized columnar query execution... And it was a large system that was not really easy for the typical R users to adopt. So the first idea was to say "Hey, let's maybe build a light version of Monedb, and integrate it with-" I think it was dplyr, or something like this... And we just let it run on the client.

But eventually, it turned out to be easier maybe to just rebuild the database system from scratch, that was actually designed to run in process, to be super-lightweight, super-easy to install, and everything. Essentially, to give the power of this vectorized query execution into the hands of data analysts.

**Chris Benson:** I'm wondering if you could -- when you talk about that being in process, and lightweight, could you describe what that means for someone that may not be familiar with the term in process? And how is that different from other databases that are not in process, that have their own processes? Can you describe a little bit of what that means?

**Till Döhmen:** So classical database systems operate in a client-server architecture, usually. You have a database server running somewhere, and you have a client that sends SQL queries, essentially, to the database server, and then the result is transferred back to the client through some kind of transfer protocol.

And one paper that Hannes and \[unintelligible 00:14:58.18\] they were working on a paper that basically benchmarked these client protocols, and it turned out that that was actually a huge bottleneck. So even when you're running Postgres on your local machine, you still have this client-server protocol bottleneck. And the way to get around this is to have the database actually running within your process, that is in that case maybe R or Python, and has access to the result set just in memory. And no transfer has to happen.

**Chris Benson:** And maybe - I'd like to just add in, for those who maybe haven't done programming and stuff in our audience, that it's expensive to go between processes... And so that database server in a different process - it takes a lot of resource to go from the process you're in off to that and back. So this puts it all into one, you might say, one little sandbox, where you're able to maximize that. Would that be a fair assessment?

**Adithya Krishnan:** \[00:16:06.27\] Yeah. So I think one of the other advantages of having this type of a model is that you can share memory between the processes. So just to go a little bit into the technical aspects of this, is that the bottleneck that Till was explaining was more like the data transfer bottleneck. But in this case, when it's running within the process, you can share the same memory, you can share the variables that you're crunching inside. Let's say a Python script that you're crunching a variable, and then you have access to the variable inside your database as well, for an example.

And this makes it super-powerful for the developer, for the developer experience as well. And I guess one of the things that, apart from the database itself being super-fast, the developer experience of using DuckDB is so awesome in that sense that I guess that has also led to the success of it.

**Break**: \[00:16:59.21\]

**Daniel Whitenack:** So Adithya, you were just describing the developer experience, which I would definitely say is kind of fitting that magical experience that you alluded to with DuckDB... And maybe just to give a sense to people - when I was initially exploring this, similar to some of the experiences that you all talked about, I would encourage our listeners to go out and install DuckDB locally and try something, because it is a really interesting experience, especially for those that have worked with traditional database systems in the past, and all of a sudden... So you kind of install DuckDB locally, import it as a library, then you can query, point to CSV files or JSON files, or Parquet files, or even a database like a Postgres database, or data stored in an S3 bucket, and you have this consistent then SQL interface that's familiar, that you can do queries over that data.

So I don't know, maybe one of you could describe some of the -- just to give people a sense of the use cases for DuckDB, maybe on one side where it's like the primary, or the key, or the most often occurring use cases that you see people grabbing DuckDB and using it for, and then maybe on the other side, just to kind of help people understand where it fits, maybe where it wouldn't be as relevant, if you have any of those thoughts.

**Adithya Krishnan:** I can give a brief overview of this. Some of the biggest users of DuckDB come from the Python ecosystem, which means that it's being a stand-in for a data frame, for example. And one of the advantages of using DuckDB is that it's really fast on aggregates, and for the Python ecosystem it helps with standing in for a data frame to be used with other ML libraries, for example.

So that's one part of the ecosystem. And the other part of the ecosystem is for a data engineer to be able to pull in data from different sources - like you said, Postgres, from CSV - and to be able to join those different datasets. Joins are really good with DuckDB as well, and to create transformed datasets is also pretty useful.

And on the third ecosystem, for a data analyst who is writing SQL, and one of the really nice aspects of DuckDB is the SQL dialect itself. It's pretty flavored, that you have a lot of DuckDB functions that makes data cleaning easy, data transformation easy. For example, we also have a dialect that says FROM TABLE, and that's just going to show you the table. Instead of going SELECT STAR FROM TABLE, you can go FROM TABLE, and that will just fetch data from that table. So there are these flavors of dialect for DuckDB that makes it nice.

**Chris Benson:** I was also looking through the DuckDB website and stuff, and I know it runs on kind of all the major platforms and architectures, and you support a variety of languages on it... I'm curious, because -- I'm asking a question to my own interest, selfishly, as Dan would say... Do you support kind of embedded environments, and kind of on the edge, that kind of stuff, where you find it embedded, and operating, where it's not necessarily on a cloud server on one of the major platforms? Is that a typical use case?

**Adithya Krishnan:** That is one of the good use cases for DuckDB. Since it's the in-process protocol that it has for running DuckDB, it can run wherever you run Python, or R, or anywhere. And they've also optimized it to run in different architectures as well.

So this makes it possible. And to kind of go beyond that, you can also run it in the browser. So any edge environment, you can run it.

Of course, there's a lot of optimization for -- there are a lot of edge environments at the moment. Not everything is optimized to run DuckDB, but I guess it's also moving towards being run in every edge environment as well.

**Daniel Whitenack:** \[00:23:54.21\] Some of our listeners might be curious why a person like me is sort of living day-to-day in the AI world is super-excited to talk about DuckDB. I mean, certainly I have a past in more broadly data science, and this is pain I've felt over time... But also, there's a very relevant piece of this that intersects with the needs of the AI community more broadly, and the workflows that they're executing. And one of those is - where I kind of started getting into this is in these sort of dashboard-killing AI apps that people are trying to build, in the sense that "Hey--" Another pain of mine as a data scientist in my life is building dashboards. Because you always build them, and they never answer the questions that people actually have... And so there's this real desire to have like a natural language question input, and you can then compute very quickly the answer to that natural language question by using the LLM to generate a SQL query to a number of data sources.

But then when you start thinking about "Oh, well, now I have these CSV files that people have uploaded into a chat interface, or I have these types of databases that I need to connect to, or I have this data in S3 buckets", and my answer could come from these different places, all of a sudden this kind of rich SQL dialect that you talked about, that's very quick, and can run with a standardized API across those sources becomes incredibly intriguing for me. Transparently, that's how I sort of like got into this, is I'm thinking of all of these sources of data that I could answer questions out of using an LLM... But how do I standardize a fast interface to all of these diverse sets of data, and also do it in a way that is easy to use from a developer's perspective? But I also know that you all see much more than I do, and maybe that is an entry point that you're seeing. I'm wondering if one of you could talk a little bit more broadly of how the problems that DuckDB is solving, and the problems that your customers are looking at are intersecting with this rapidly developing world of AI workflows.

**Till Döhmen:** I mean, one way to describe DuckDB is it's the SQLite for analytics. So it is basically a very easy way, a very developer-friendly way to achieve what you just described. If I want to create a demo for my new text to SQL model, if I use DuckDB for it, I can even make a completely WASM-based demo out of it, for example. I don't have any issues with CSV upload. There might be databases where I have to specify the limiter of the file that the user uploads... So I would have to show a dialogue to my user where it says "Oh, that's comma-separated and it has a header row", and so on. With DuckDB, it just works. So it takes away some of the edges you might have with other databases.

And on top of that, as you said, it integrates with different storage backends. It can read from S3, it can read from HTTP... When I see an interesting file on, let's say Hugging Face or GitHub, I just run read CSV from this URL, and I have the dataset locally in my CLI or in my Python.

Furthermore, when I have, say a Python environment, I start a Colab notebook, and I create some data frames. Then with DuckDB, I can just read those data frames. I've seen very cool demos of people basically using Text-to-SQL for analytics on Pandas data frames. And under the hood, it's just DuckDB sitting there, and basically reading straight from those Pandas data frames, which by the way is one of the other benefits of shared memory of in process.

\[00:28:17.10\] It's not only for fetching results, it's also for reading data straight from the process, so in that case from Pandas. That's very exciting. I'm happy to talk more about Text-to-SQL. We have had a project about that at MotherDuck. But yeah.

**Daniel Whitenack:** Yeah, and maybe also - before we get into maybe some of those stories, I think that that's one side of it, is the integration of this analytics piece into AI workflows. But then also, if I'm not mistaken, there is sort of vector search capabilities within DuckDB as well. I don't know if one of you could speak to that.

**Adithya Krishnan:** Yeah, that's one of the exciting aspects of DuckDB as well. So if I could take a step back and think about other ecosystems where let's say Postgres has been shining a lot... Postgres has exploded into the kind of possibilities that you can do because it has an amazing extension mechanism, where you could add extensions and capabilities of Postgres. And in a similar way, DuckDB has an extension mechanism that you have access to the internal workings of DuckDB, and you could add more workflows on top of what DuckDB can do.

DuckDB has these capabilities of doing vector search, for example, and it also has hybrid search, where you also have full-text search, and vector search that you could put together to create hybrid search. One of the ways it does is that it has a really nice data type. I can go into the rabbit hole of the inner workings of how they make this happen, which is also pretty exciting... But one of the things that they make this possible is to provide an array data type where you can have an array of floating points, and then you can store this as a data type, and then that eventually becomes an embedding vector that you can do cosine similarity against.

So that is to do an embedding-based search. Then you can also have full-text search, where you can create an inverted index of keywords to your documents, and you can search across your keywords to find your ideal documents and rank them according to the score. And then you could fuse both of these scores from embedding search and from full-text search to have like a hybrid search. So yeah, so all of these are possible, and they're very accessible.

**Break**: \[00:30:44.26\]

**Daniel Whitenack:** So Till, you were starting to get into even some of the things now that you're doing at MotherDuck, on top of DuckDB... I'm wondering -- hopefully we can get to some of those use cases or the things that you've been doing with customers, or internally... But I'm wondering, before we do that, I see also this sort of story about DuckDB's efficiency, but with this kind of multiplayer aspect as part of what you're doing at DuckDB. So maybe one of you could describe kind of -- now I think we have a sense of what DuckDB is, and it's this free thing that is open, and I can pull down, I can install, I can run it very quickly, run it on my laptop, run it in my browser, do these analytics queries... So now kind of describe maybe a little bit of how you're taking that further with MotherDuck, and how you're thinking about some of the enterprise use cases.

**Till Döhmen:** I like to describe MotherDuck as giving your DuckDB a cloud companion. So it's easy to associate "Okay, we bring MotherDuck to the cloud", which is one way how we describe ourselves as well, to associate that with "We provide infinite scale-up in the cloud. You give us a workload, and we start how many hundred DuckDBs in the background, that in a task-like fashion, let's say process your data concurrently." But actually, one of the hypotheses that MotherDuck is based on, or that the company was founded on is that actually single-node compute, which means one DuckDB database, with nowadays hardware, cloud hardware, actually gets you very, very, very far. So when your local compute resources reach a limit, you have single-cloud instances with up to - how much is it? 24 terabyte of memory? That's relatively big data.

So that's one aspect... So scaling up with one cloud company in DuckDB. Another aspect is collaboration. So once you're connected to a cloud instance, you can have shared context with other users in your organization. You can create shared datasets, you can have shared notebooks, and so on and so forth. And with that, of course, comes all the enterprise SOC2 kind of things that some of the enterprise customers require to adopt towards DuckDB.

**Chris Benson:** \[00:36:00.20\] I'm curious if you could -- you really captured my imagination with that description. Because by drawing, for instance, with kind of the old school Postgres things that people would do with that... And you just talked about having many DuckDB instances operating concurrently. What kinds of problems, and kind of grounding it in a practical way from a user's perspective, what kind of problems do you see people solving with that kind of architecture and that new capability that they may not have historically had over the years with previous database capabilities on other platforms? What new sets of concerns can they address now with those?

**Till Döhmen:** I would come from the perspective on this that there are a lot of companies out there that when they want to go to the cloud with their analytics workload, they have relatively limited choices. One of those choices is like Snowflake, or Databricks. And of course, those systems are optimized for big data scale. But then one of our observations is that a lot of companies actually don't have that amount of data when they run queries; or they might have big data, but the queries they are running only access a very small subset of their data, for example. You run monthly reports... They don't touch your entire historic dataset. So those companies might want to have something that is first easier to use, easier to set up, and that's also more cost-efficient than other existing solutions.

**Adithya Krishnan:** One of the things that we haven't touched upon in this yet is kind of how MotherDuck and DuckDB go hand in hand with like the remote and the local aspect, where you have on your local and your remote the same client, so that you're actually running the same thing... So it's easy to go from one place to the other, doing the same thing.

And what MotherDuck also provides is a dual execution, where your local DuckDB, if you're running it locally, can communicate with your remote MotherDuck, and execute seamlessly between both. For example, a query where you have a table in your local DuckDB and you want to join it with a remote DuckDB, you can join both of these tables together to run an aggregate. And then there's a query optimization that we run where we transfer the data, which was required from the remote to your local or from your local to remote, and execute it intelligently in a way, if I could say that. And this kind of opens up new opportunities in like the dual execution aspect of running local and the remote with the same client.

**Chris Benson:** I'm curious - again, selfish question - as you're doing that and you have the local version and the remote version, the connection between the two there... What does that look like? Is it something that if they're widely separated, if MotherDuck's in the cloud and I'm out on a device that's not cloud-based - is that efficient communication? How do you all handle those different types of use cases?

**Till Döhmen:** Yeah, so one of the principles of this dual execution is to reduce the amount of data that has to be transferred as much as possible. One of the use cases, for example, is I have a really large dataset on S3, and I want to join it with a small table that I have on my notebook. So in that case, an optimizer, a query optimizer, will make the decision to, instead of downloading the one-terabyte dataset to your local device and doing the join there, to instead upload your small local file to the cloud worker, and do the processing there. So that saves, in that case, a lot of bandwidth.

\[00:40:06.25\] The same with a filter pushdown. I query a large dataset on S3 again, and the transfer only has to happen for the filter. You can get something similar with DuckDB as well if the data is partitioned. So DuckDB has clever ways to optimize remote file access as well, without MotherDuck. But the thing you get with MotherDuck is it even filters the data if your data is not partitioned, because the cloud worker still takes care of doing the bulk of the work, and only gives you the results you actually want and need.

**Daniel Whitenack:** A lot of what we've talked about are the features of DuckDB, and then what MotherDuck is adding on that, and also how that intersects with AI workflows, like the text-to-SQL case, or the RAG case where we're doing vector or semantic search, or we're doing hybrid search... All of those things are super-relevant to people building their AI workflows, but I also find it interesting that -- I see, Till, you wrote one of the blog posts that I'm looking at now, which is like you're also thinking as a company about how to use AI intelligently in your own product as well, for the users of your product who are maybe technical users, they're building their own workflows, but also you have sort of AI integrated into some of the features of that. I'm looking at this fix-it feature... So I'm wondering if you could talk a little bit about that, how this is both you're enabling AI developers, but also you are definitely integrating this technology as well; at least that's how it seems.

**Till Döhmen:** Yeah. As Adithya mentioned, one of the big appeals of DuckDB is the simplicity. That's what brings a lot of users to DuckDB. I think that simplicity can be extended towards usage of AI, to a certain extent. Usage of AI in the context of data analytics, data management. And there are multiple aspects to that.

On one hand, there's the user experience side of things. So how can we make it easier for people to write SQL? And I think the answer to that is not only text to SQL. And part of that story is fix-it.

So one of our main aims with fix-it was to keep it -- basically make it non-intrusive and not interrupting your flow of writing SQL, while still being helpful when it triggers. And I think Cursor, for example, is an excellent example of integrating AI into IDEs, or into the workflow of software developers. And in our case, we have to think more about data engineers, and data analysts, and I think it's a super-exciting time for those kind of things.

I think MotherDuck is a particularly interesting place to work on those kind of things, because one of the unique advantages that we have is we have an actual database running on the client side, in the browser of the user. If someone is using our web UI, that user has actually DuckDB running in their browser, that can do parsing, binding, and that gives us so much information about the current state of the query that the user is writing. And fix-it only scratches the surface of what is possible in terms of SQL writing assistance in that sense.

**Chris Benson:** So I'm curious, as we start winding up, you really got me thinking about use cases that I had not thought about before, and all the things I might be able to do here... So I'm a little bit like a kid in a candy store. I've got to ask you, and I'd like each of you to take a swing at it... It's pretty cool what you've talked about today in terms of what is possible for us. How are you thinking about the future? What are the new, cool things that you have in mind?

\[00:44:12.24\] I often say like when you're kind of not necessarily working hard on a problem, but you're kind of chilling out at the end of the day, and your mind is just wandering in free form, and you're thinking "Boy, what if we could do this? I could imagine that, and I can kind of see a path forward to get there." How are each of you thinking about MotherDuck and DuckDB in terms of what the future might offer? If you want to kind of get out there and wax poetic a little bit, and - it doesn't have to be grounded in current work, but more in imagination and aspiration.

**Adithya Krishnan:** One of the things that I really like about the current state of AI is how good the local models are, the small models that you can run locally... And there's a great ecosystem out there building on top of that. One of the things that I see with the local models - of course, they hallucinate. But to prevent hallucination, you can use a really nice RAG mechanism to put context into those local models. And these local models could be on the edge as well. It could be on your local laptop, it could be on the edge. And knowledge bases are essentially created to kind of prevent these kind of hallucinations. And one wasteful aspect of creating knowledge bases is that everybody's creating very similar knowledge bases. And what if there could be a mechanism where we could share these knowledge bases? A user could create a knowledge base and they could share a knowledge base.

And one of the imaginative worlds that I'm dreaming is how MotherDuck could be there to do these kind of shareable knowledge bases, where you essentially have a world of remote knowledge bases out there in your remote tables, and then you have a local DuckDB client there that helps you pull a knowledge base that you want, use the local knowledge base, augment your local model with the relevant context for your current question... And then whenever you don't want the knowledge base, you could also drop the knowledge base. And that's like having a remote knowledge base repository and pull whatever you want. This is like one of the dreams that I think about how MotherDuck could be -- how MotherDuck and DuckDB could be useful for this.

And another aspect of talking about knowledge bases and RAG applications is that not all applications and workflows require a real-time database to build agents on top of them. And some of these agents could be running as background agents, that do some workflow once every day. And instead of having a real-time database for that, what if you could provide a very lightweight, analytical engine that's quite cheap to run locally as well, and you could offload some work to the remote cloud? So this is another thing that keeps me excited at night, to think about what could be these kinds of use cases... But yeah, these are the two use cases that I am quite excited about.

**Till Döhmen:** Yeah, maybe I can add two things. One thing that actually connects to that and that is bringing AI and machine learning capabilities more into the database... So one of the things we've seen in the past is that the inference costs of language models have dropped quite significantly compared to two years ago. It's now I think only 2% of the price for inference with GPT-4 mini compared to GPT-3. And that actually makes it possible to run a language model inference on your tables, and also to do things like embedding compute on your tables. And SQL is just a really convenient user interface for that. So we added this embedding function some time ago that works really well together with the vector search. So you can basically do embedding-based search only in SQL. Now we're adding the prompting capabilities, so you can do language model-based data wrangling in your database, and that together with local models, in this hybrid execution model; we say "Okay, we do part of the work locally." Maybe if you have a GPU, do part of the embedding inference locally. If you want to do it faster, do it in the cloud, with a few A100s. And again, everything is in SQL.

**Daniel Whitenack:** That's awesome. Yeah, well, thank you both for taking time out of your analytics AI database work to come talk to us. This has been super-amazing, and I would definitely encourage people out there - please, please, please go try out some things. Try out some examples with DuckDB. Check out the MotherDuck website, and some of the great blog posts content that they have there, or examples, or things that they're doing. Check it out, because it's definitely a really wonderful thing that you can add into your AI stack, and think about and experiment with.

So thank you so much, Till and Adithya, for joining. It's been a pleasure.

**Till Döhmen:** Thank you guys for having us.

**Adithya Krishnan:** Thank you guys. It was pretty awesome to be here.
